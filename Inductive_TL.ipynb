{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sundrop03/TL/blob/main/Inductive_TL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eeccc18",
      "metadata": {
        "id": "7eeccc18"
      },
      "source": [
        "# Inductive transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "148af369",
      "metadata": {
        "id": "148af369"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00772ed8",
      "metadata": {
        "id": "00772ed8"
      },
      "source": [
        "This notebook demonstrates the applications of inductive transfer learning discussed in the paper [\"Transfer Learning in the Actuarial Domain\"]().\n",
        "\n",
        "We can learn a claims frequency prediction model using a large data set with fire and robbery claims as our labels. Then the learnings from this model can be transferred to be used for injury claims frequency prediction for a smaller data set."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f45e94a0",
      "metadata": {
        "id": "f45e94a0"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22e7bb2c",
      "metadata": {
        "id": "22e7bb2c"
      },
      "source": [
        "For transfer learning, we need a source dataset to learn from and a target dataset to transfer the learnings to.\n",
        "\n",
        "Therefore, two datasets are used for the application.\n",
        "\n",
        "For the source dataset, we use the Brazilian automobile claims data set that can be accessed [online](http://www2.susep.gov.br/menuestatistica/Autoseg/) and is also available from the [CASdatasets](https://github.com/dutangc/CASdatasets). The target dataset is the Singapore automobile claims data that is also available in the CASdatasets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9f74f12",
      "metadata": {
        "id": "d9f74f12"
      },
      "source": [
        "We start with loading the necessary packages that include various metrics and loss functions used in constructing the neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1013e2a0",
      "metadata": {
        "id": "1013e2a0"
      },
      "outputs": [],
      "source": [
        "#load packages\n",
        "import pandas as pd\n",
        "import xlsxwriter\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_poisson_deviance\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.backend import exp\n",
        "from tensorflow.keras.models import Sequential, Model, save_model\n",
        "from tensorflow.keras.layers import Input, Reshape, Dense, Activation, Flatten, Concatenate, Embedding, BatchNormalization, Dropout, Add\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam, Nadam, SGD, Adamax, Adagrad\n",
        "from tensorflow.keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace83354",
      "metadata": {
        "id": "ace83354"
      },
      "source": [
        "The source data is preprocessed before it is read into the python environment.\n",
        "\n",
        "Instances with blank values for 'Gender' and 'VehModel' are removed. The blank values for corporate 'DrivAge' are filled proportionally based on the age distribution of the whole data. A possible alternative can be using a nearest neighbor algorithm to fill the empty values. Instances with values larger than 1 for the exposure feature are removed.\n",
        "\n",
        "Features that are not available in the target data are removed from the source data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33cd8453",
      "metadata": {
        "id": "33cd8453"
      },
      "source": [
        "The original source data has 23 features available of which we use features on gender, exposure, vehicle age, vehicle type, driver age, and the number of claims.\n",
        "\n",
        "These features are pre-processed to match the features of our target data. Gender consists of male, female, and corporate. Driver age is used to categorize each instance into 'AgeCat' which has 5 age groups where 0 is the youngest and 4 is the oldest age group. Based on the Vehicle year feature, we can categorize each observation into 'AutoAge0', 'AutoAge1', 'AutoAge2', 'AutoAge','VAgecat', and 'VAgeCat1'. Information on the vehicle model and vehicle group is used to determine the 'VehicleType' which has 4 categories which are A(auto), M(motorcycle), O(others), and T(truck).\n",
        "ClaimNbRob and ClaimNbFire represent the number of claims due to robbery and fire. The sum of these two features is renamed as Clm_Count.\n",
        "\n",
        "Now we have 11 final features in our source data which are Gender, VehicleType, Clm_Count, Exp_weights, AgeCat, AutoAge0, AutoAge1, AutoAge2, AutoAge, VAgeCat, and VAgecat1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d27c338d",
      "metadata": {
        "id": "d27c338d"
      },
      "source": [
        "Let's load our source data and check:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sundrop03/TL.git"
      ],
      "metadata": {
        "id": "JD45jEqY1tNw"
      },
      "id": "JD45jEqY1tNw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc80ef5",
      "metadata": {
        "id": "2fc80ef5"
      },
      "outputs": [],
      "source": [
        "mysource1= pd.read_excel(\"TL/Source_dataBS1.xlsx\")\n",
        "mysource2= pd.read_excel(\"TL/Source_dataBS2.xlsx\")\n",
        "mysource3= pd.read_excel(\"TL/Source_dataBS3.xlsx\")\n",
        "mysource4= pd.read_excel(\"TL/Source_dataBS4.xlsx\")\n",
        "mysource5= pd.read_excel(\"TL/Source_dataBS5.xlsx\")\n",
        "\n",
        "mysource = pd.concat([mysource1, mysource2, mysource3, mysource4, mysource5])\n",
        "print(mysource)\n",
        "mysource.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc156772",
      "metadata": {
        "id": "fc156772"
      },
      "source": [
        "All features except Exp_weights are categorical variables that need to be encoded to be used for machine learning methods.\n",
        "\n",
        "Vehicle type and Gender are categorical variables, but without ordinal characteristics, so we apply one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b554ee1a",
      "metadata": {
        "id": "b554ee1a"
      },
      "outputs": [],
      "source": [
        "#process source data for use\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "mysource1 = pd.DataFrame(encoder.fit_transform(mysource[['Gender']]).toarray())\n",
        "mysource2 =  mysource.join(mysource1)\n",
        "mysource2.drop('Gender', axis=1, inplace=True)\n",
        "mysource2.columns = ['VehicleType', 'Clm_Count', 'Exp_weights', 'AgeCat','AutoAge0',\n",
        "                     'AutoAge1','AutoAge2','AutoAge','VAgeCat', 'VAgecat1', 'Corporate', 'Female', 'Male']\n",
        "\n",
        "encoder1 = OneHotEncoder(handle_unknown='ignore')\n",
        "mysource3 = pd.DataFrame(encoder1.fit_transform(mysource2[['VehicleType']]).toarray())\n",
        "final_source = mysource2.join(mysource3)\n",
        "final_source.drop('VehicleType', axis=1, inplace=True)\n",
        "final_source.columns = ['Clm_Count', 'Exp_weights', 'AgeCat','AutoAge0','AutoAge1',\n",
        "                        'AutoAge2','AutoAge','VAgeCat', 'VAgecat1', 'Corporate', 'Female', 'Male', 'Auto', 'Motorcycle', 'Other', 'Truck']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa34bc95",
      "metadata": {
        "id": "fa34bc95"
      },
      "source": [
        "Now let's load our target data and apply the same encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c4d9058",
      "metadata": {
        "id": "9c4d9058"
      },
      "outputs": [],
      "source": [
        "mytarget= pd.read_excel(\"TL/Target_dataB.xlsx\")\n",
        "print(mytarget)\n",
        "mytarget.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5c4efd8",
      "metadata": {
        "id": "b5c4efd8"
      },
      "outputs": [],
      "source": [
        "#process target data for use\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "mytarget1 = pd.DataFrame(encoder.fit_transform(mytarget[['Gender']]).toarray())\n",
        "mytarget2 =  mytarget.join(mytarget1)\n",
        "mytarget2.drop('Gender', axis=1, inplace=True)\n",
        "mytarget2.columns = ['VehicleType', 'Clm_Count', 'Exp_weights', 'AgeCat','AutoAge0',\n",
        "                     'AutoAge1','AutoAge2','AutoAge','VAgeCat', 'VAgecat1', 'Corporate', 'Female', 'Male']\n",
        "\n",
        "encoder1 = OneHotEncoder(handle_unknown='ignore')\n",
        "mytarget3 = pd.DataFrame(encoder1.fit_transform(mytarget2[['VehicleType']]).toarray())\n",
        "final_target = mytarget2.join(mytarget3)\n",
        "final_target.drop('VehicleType', axis=1, inplace=True)\n",
        "final_target.columns = ['Clm_Count', 'Exp_weights', 'AgeCat','AutoAge0','AutoAge1',\n",
        "                        'AutoAge2','AutoAge','VAgeCat', 'VAgecat1', 'Corporate', 'Female', 'Male', 'Auto', 'Motorcycle', 'Other', 'Truck']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebe6c1ee",
      "metadata": {
        "id": "ebe6c1ee"
      },
      "source": [
        "## Setting seeds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbc96f2b",
      "metadata": {
        "id": "cbc96f2b"
      },
      "source": [
        "We can set seeds for reproducibility, but exact reproduction of results is hard to achieve due to randomness in the sampling, cross-validation, dropout layers, and methods. This doesn't change the overall findings from the results.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8817003d",
      "metadata": {
        "id": "8817003d"
      },
      "outputs": [],
      "source": [
        "seed_val = 10579\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_val)\n",
        "import random\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "tf.random.set_seed(seed_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf737918",
      "metadata": {
        "id": "bf737918"
      },
      "source": [
        "## Defining the neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34dbe3fe",
      "metadata": {
        "id": "34dbe3fe"
      },
      "source": [
        "We define the structure of our neural network that will be used throughout the applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "265b0bac",
      "metadata": {
        "id": "265b0bac"
      },
      "outputs": [],
      "source": [
        "#define NN model\n",
        "\n",
        "def NN_model(input_shape=(14,)):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(28,input_shape=input_shape,kernel_initializer = 'GlorotNormal'))\n",
        "    model.add(BatchNormalization()) #BatchNormalization\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Activation('tanh'))\n",
        "    model.add(Dense(21,kernel_initializer = 'GlorotNormal'))\n",
        "    model.add(BatchNormalization()) #BatchNormalization\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Activation('tanh'))\n",
        "    model.add(Dense(14,kernel_initializer = 'GlorotNormal'))\n",
        "    model.add(BatchNormalization()) #BatchNormalization\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Activation('tanh'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.add(Dense(1, activation='exponential',trainable = False))\n",
        "    model.compile(optimizer=Nadam(0.004), loss='poisson')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a396b6",
      "metadata": {
        "id": "a2a396b6"
      },
      "source": [
        "We define a sequential model which can also be defined as a functional model.\n",
        "\n",
        "The network has one input layer with 14 neurons that take the 14 features as input. These are passed through three Dense(hidden) layers with tanh activation functions that have 28, 21, and 14 neurons respectively. Then the output is combined and passed through a dense layer with 1 neuron and a linear activation function. Finally, the results are combined at the output layer with an exponential activation function and the loss function is set to 'poisson' since our outcome is the predicted claim frequency.\n",
        "\n",
        "Unlike the transductive model, we initialize the weights for each hidden layer by a Glorot normal initializer.\n",
        "\n",
        "The choice of optimizer was determined by experiments. We have considered a sgd (Stochastic Gradient Descent) with learning rate of 0.1 and momentum of 0.9 as a possible alternative. With dropouts applied to multiple layers, increasing the learning rate and momentum to a higher level provided reasonable results. But this also resulted in very large weights that showed poor performance in terms of predictions. The final choice is Nadam which is an Adam optimizer with Nesterov momentum. The learning rate is set to 0.004 which depends on the batch size used for the training.\n",
        "\n",
        "To control for overfitting, we apply regularization such as batch normalization and dropout to the first three dense layers. There are different opinions regarding the order of applying batch normalization and dropout suggesting dropout coming first. Also, it is quite common to use higher probabilities (e.g., 80\\%) used in dropout for input layers and relatively lower probabilities (e.g., 50\\%) for other layers. But these differ by application and the proposed order and hyperparameters are determined through experiments.\n",
        "\n",
        "Having a large dropout probability works relatively well when training a neural network on a single dataset. But for the application of transfer learning, small dropout probablility worked better. Therefore, we set it to a constant 0.1 to retain most of the output from all layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9de2e931",
      "metadata": {
        "id": "9de2e931"
      },
      "source": [
        "## Baseline model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce54d5b2",
      "metadata": {
        "id": "ce54d5b2"
      },
      "source": [
        "We are interested in the performance difference between models with and without transfer learning. Therefore, we run a baseline model that only learns from the target data.\n",
        "\n",
        "The target data is split into 60\\% train, 25\\% validation, and 15\\% test data.\n",
        "\n",
        "The outcome of interest 'Clm_Count' should be considered with its corresponding 'Exp_weights'. In the model, we take this into account by creating bootstrap samples distributed according to the the 'Exp_weights' feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d1dcbbb",
      "metadata": {
        "id": "5d1dcbbb"
      },
      "outputs": [],
      "source": [
        "callback =tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0,patience=30,verbose=0,\n",
        "                                           mode='auto',baseline=None,restore_best_weights=True)\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "N = 1000\n",
        "BS = pd.DataFrame(columns=['PD','Error'])\n",
        "for i in range(N):\n",
        "    if (i % 200 == 0):\n",
        "        print(\"Iteration\", i)\n",
        "\n",
        "    #split training and testing data within target\n",
        "\n",
        "    scalerIBA = MinMaxScaler()\n",
        "    final_target[['AgeCat','VAgeCat', 'VAgecat1']] = scalerIBA.fit_transform(final_target[['AgeCat','VAgeCat', 'VAgecat1']])\n",
        "    Xmy = final_target.drop(columns = ['Exp_weights','Clm_Count']).values\n",
        "    ymy = final_target['Clm_Count'].values\n",
        "    vmy= final_target['Exp_weights'].values\n",
        "\n",
        "    idx_boot = np.random.choice(np.arange(len(Xmy)), len(Xmy), replace=True, p=vmy/vmy.sum())\n",
        "    Xmy_bs = Xmy[idx_boot]\n",
        "    ymy_bs = ymy[idx_boot]\n",
        "    vmy_bs = vmy[idx_boot]\n",
        "\n",
        "    rng = np.random.default_rng()\n",
        "    idx1 = rng.choice(np.arange(len(Xmy_bs)), round(len(Xmy_bs)*0.6), replace=False)\n",
        "    idx2 = np.delete(np.arange(len(Xmy_bs)),idx1)\n",
        "\n",
        "    Xtr = Xmy_bs[idx1]\n",
        "    ytr = ymy_bs[idx1]\n",
        "    vtr = vmy_bs[idx1]\n",
        "\n",
        "    Xva_my = Xmy_bs[idx2]\n",
        "    yva_my = ymy_bs[idx2]\n",
        "    vva_my = vmy_bs[idx2]\n",
        "\n",
        "    rng1 = np.random.default_rng()\n",
        "    idx3 = rng1.choice(np.arange(len(Xva_my)), round(len(Xva_my)*0.625), replace=False)\n",
        "    idx4 = np.delete(np.arange(len(Xva_my)),idx3)\n",
        "\n",
        "    Xva = Xva_my[idx3]\n",
        "    yva = yva_my[idx3]\n",
        "    vva = vva_my[idx3]\n",
        "\n",
        "    Xte = Xva_my[idx4]\n",
        "    yte = yva_my[idx4]\n",
        "    vte = vva_my[idx4]\n",
        "\n",
        "\n",
        "    model1 = NN_model()\n",
        "    model1.fit(Xtr,ytr,batch_size=512, callbacks=[callback], verbose=2,epochs=400, validation_data=(Xva,yva))\n",
        "\n",
        "    Plain_preds = model1.predict(Xte)\n",
        "    deviance = mean_poisson_deviance(yte, Plain_preds)\n",
        "    error = mean_squared_error(yte, Plain_preds)\n",
        "\n",
        "    BS.loc[i, ['PD']] = deviance\n",
        "    BS.loc[i, ['Error']] = error\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "print((end - start)/60.0, \"min elapsed.\")\n",
        "\n",
        "print(BS.mean())\n",
        "print(BS.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9de4f55",
      "metadata": {
        "id": "d9de4f55"
      },
      "source": [
        "We set the model to have 1,000 iterations to produce 1,000 sets of results. For each iteration, we calculate the MPD(Mean Poisson Deviance) and MSE(Mean Squared Error) to determine the performance of the model. If the predictions are closer to the true outcome, then the deviation decreases, which indicates an improvement in the predictions. Our problem is a Poisson regression problem, so using MPD with a traditional regression metric MSE will be a reasonable choice."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86803971",
      "metadata": {
        "id": "86803971"
      },
      "source": [
        "The batch size is determined through a balance between runtime and gain in performance. Usually a small batch size of 32 or 64 with a learning rate of 0.001 provides a good prediction, but it greatly increases the runtime. We increased the batch size to 512 and the learning rate of the optimizer was increased to 0.004, which is by a factor of $\\sqrt{16}$. Increasing the learning rate by a factor of the increase in batch size resulted in less runtime with similar predictive performance.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10b33731",
      "metadata": {
        "id": "10b33731"
      },
      "source": [
        "The baseline model has a MPD of 0.4478(0.0530) with a MSE of 0.0984(0.0233). Values in the parentheses are standard deviations.\n",
        "\n",
        "Now we have the baseline model! Let's move on to apply inductive transfer learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0176c6e",
      "metadata": {
        "id": "f0176c6e"
      },
      "source": [
        "## Parameter-based methods"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5500636f",
      "metadata": {
        "id": "5500636f"
      },
      "source": [
        "For inductive transfer learning, the source data and target data have the same domain but related tasks. In our case, the source task is to predict auto claims due to robbery and fire, while the target task is to predict auto injury claims.\n",
        "\n",
        "We experiment with a widely used parameter-based approach. The source data is split into 80\\% training data, 12.5\\% validation data, and 7.5\\% test data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a554b3c",
      "metadata": {
        "id": "1a554b3c"
      },
      "source": [
        "### Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd6cc7c",
      "metadata": {
        "id": "6bd6cc7c"
      },
      "source": [
        "Transfer learning is applied in two steps.\n",
        "\n",
        "First, we train the baseline model on the large source data and save the model which includes the structure and the weights learned from the training. Out of the 1,000 iterations, we choose the best model to transfer based on the metrics.\n",
        "\n",
        "Second, we load the saved model, remove the output layer, and set the remaining layers to 'non-trainable' also known as\n",
        "'freezing' the layers. Then a new output layer is added to the model to adapt the learning from the source task to the target task.\n",
        "\n",
        "If the source task and target task are very different, then we need to freeze fewer layers from the source model and update them during the target training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13fa27d2",
      "metadata": {
        "id": "13fa27d2"
      },
      "source": [
        "### Step 1: Training source model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70794554",
      "metadata": {
        "id": "70794554"
      },
      "source": [
        "For each of the 1,000 iterations, we train the source model, record the metrics, and save the model itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c208aef3",
      "metadata": {
        "id": "c208aef3"
      },
      "outputs": [],
      "source": [
        "callback =tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0,patience=30,verbose=0,\n",
        "                                           mode='auto',baseline=None,restore_best_weights=True)\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "N = 1000\n",
        "BS = pd.DataFrame(columns=['PD','Error'])\n",
        "for i in range(N):\n",
        "    if (i % 200 == 0):\n",
        "        print(\"Iteration\", i)\n",
        "\n",
        "    #split training and testing data within source\n",
        "\n",
        "    scalerIBA1 = MinMaxScaler()\n",
        "    final_source[['AgeCat','VAgeCat', 'VAgecat1']] = scalerIBA1.fit_transform(final_source[['AgeCat','VAgeCat', 'VAgecat1']])\n",
        "    Xmy = final_source.drop(columns = ['Exp_weights','Clm_Count']).values\n",
        "    ymy = final_source['Clm_Count'].values\n",
        "    vmy= final_source['Exp_weights'].values\n",
        "\n",
        "    idx_boot = np.random.choice(np.arange(len(Xmy)), len(Xmy), replace=True, p=vmy/vmy.sum())\n",
        "    Xmy_bs = Xmy[idx_boot]\n",
        "    ymy_bs = ymy[idx_boot]\n",
        "    vmy_bs = vmy[idx_boot]\n",
        "\n",
        "    rng = np.random.default_rng()\n",
        "    idx1 = rng.choice(np.arange(len(Xmy_bs)), round(len(Xmy_bs)*0.8), replace=False)\n",
        "    idx2 = np.delete(np.arange(len(Xmy_bs)),idx1)\n",
        "\n",
        "    Xtr = Xmy_bs[idx1]\n",
        "    ytr = ymy_bs[idx1]\n",
        "    vtr = vmy_bs[idx1]\n",
        "\n",
        "    Xva_my = Xmy_bs[idx2]\n",
        "    yva_my = ymy_bs[idx2]\n",
        "    vva_my = vmy_bs[idx2]\n",
        "\n",
        "    rng1 = np.random.default_rng()\n",
        "    idx3 = rng1.choice(np.arange(len(Xva_my)), round(len(Xva_my)*0.625), replace=False)\n",
        "    idx4 = np.delete(np.arange(len(Xva_my)),idx3)\n",
        "\n",
        "    Xva = Xva_my[idx3]\n",
        "    yva = yva_my[idx3]\n",
        "    vva = vva_my[idx3]\n",
        "\n",
        "    Xte = Xva_my[idx4]\n",
        "    yte = yva_my[idx4]\n",
        "    vte = vva_my[idx4]\n",
        "\n",
        "    model_TL = NN_model()\n",
        "    model_TL.fit(Xtr,ytr,batch_size=512, callbacks=[callback], verbose=2,epochs=400, validation_data=(Xva,yva))\n",
        "\n",
        "    Plain_preds = model_TL.predict(Xte)\n",
        "    deviance = mean_poisson_deviance(yte, Plain_preds)\n",
        "    error = mean_squared_error(yte, Plain_preds)\n",
        "\n",
        "    BS.loc[i, ['PD']] = deviance\n",
        "    BS.loc[i, ['Error']] = error\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "print((end - start)/60.0, \"min elapsed.\")\n",
        "\n",
        "save_model(model_TL, \"model1.h5\")\n",
        "\n",
        "writer = pd.ExcelWriter('InSource.xlsx', engine='xlsxwriter')\n",
        "BS.to_excel(writer, sheet_name='welcome', index=False)\n",
        "writer.save()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c9b413b",
      "metadata": {
        "id": "5c9b413b"
      },
      "source": [
        "After the run, we have 1,000 models and a spreadsheet containing the deviance and error values of those models.\n",
        "\n",
        "Based on the deviance and error value, we choose 'model1' as the model to be transffered to the target."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5efecf45",
      "metadata": {
        "id": "5efecf45"
      },
      "source": [
        "### Step 2: Transferred model with one unfrozen dense layer (Transfer 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "832ca079",
      "metadata": {
        "id": "832ca079"
      },
      "source": [
        "We load the chosen 'model1', remove the output layer, freeze the remaining layers. Then we add a new output layer that is trainable. In other words, we are using all the parameters from the source model except the last layer. This last layer is used to adapt the model from the source task to the target task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13e365e8",
      "metadata": {
        "id": "13e365e8"
      },
      "outputs": [],
      "source": [
        "model = load_model('model1.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c25fcd7",
      "metadata": {
        "id": "6c25fcd7"
      },
      "outputs": [],
      "source": [
        "model1 = Sequential()\n",
        "\n",
        "for layer in model.layers[:-1]:\n",
        "    model1.add(layer)\n",
        "for layer in model1.layers:\n",
        "    layer.trainable=False\n",
        "\n",
        "model1.add(Dense(1, activation='exponential',name='dense_4'))\n",
        "model1.compile(optimizer=Nadam(0.004), loss='poisson')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cce88566",
      "metadata": {
        "id": "cce88566"
      },
      "source": [
        "This new model is trained on the target data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "371dcaa2",
      "metadata": {
        "id": "371dcaa2"
      },
      "outputs": [],
      "source": [
        "callback =tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0,patience=30,verbose=0,\n",
        "                                           mode='auto',baseline=None,restore_best_weights=True)\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "N = 1000\n",
        "BS = pd.DataFrame(columns=['PD','Error'])\n",
        "for i in range(N):\n",
        "    if (i % 200 == 0):\n",
        "        print(\"Iteration\", i)\n",
        "\n",
        "    #split training and testing data within target\n",
        "    scalerITA1 = MinMaxScaler()\n",
        "    final_target[['AgeCat','VAgeCat', 'VAgecat1']] = scalerITA1.fit_transform(final_target[['AgeCat','VAgeCat', 'VAgecat1']])\n",
        "    Xmy = final_target.drop(columns = ['Exp_weights','Clm_Count']).values\n",
        "    ymy = final_target['Clm_Count'].values\n",
        "    vmy= final_target['Exp_weights'].values\n",
        "\n",
        "    idx_boot = np.random.choice(np.arange(len(Xmy)), len(Xmy), replace=True, p=vmy/vmy.sum())\n",
        "    Xmy_bs = Xmy[idx_boot]\n",
        "    ymy_bs = ymy[idx_boot]\n",
        "    vmy_bs = vmy[idx_boot]\n",
        "\n",
        "    rng = np.random.default_rng()\n",
        "    idx1 = rng.choice(np.arange(len(Xmy_bs)), round(len(Xmy_bs)*0.6), replace=False)\n",
        "    idx2 = np.delete(np.arange(len(Xmy_bs)),idx1)\n",
        "\n",
        "    Xtr = Xmy_bs[idx1]\n",
        "    ytr = ymy_bs[idx1]\n",
        "    vtr = vmy_bs[idx1]\n",
        "\n",
        "    Xva_my = Xmy_bs[idx2]\n",
        "    yva_my = ymy_bs[idx2]\n",
        "    vva_my = vmy_bs[idx2]\n",
        "\n",
        "    rng1 = np.random.default_rng()\n",
        "    idx3 = rng1.choice(np.arange(len(Xva_my)), round(len(Xva_my)*0.625), replace=False)\n",
        "    idx4 = np.delete(np.arange(len(Xva_my)),idx3)\n",
        "\n",
        "    Xva = Xva_my[idx3]\n",
        "    yva = yva_my[idx3]\n",
        "    vva = vva_my[idx3]\n",
        "\n",
        "    Xte = Xva_my[idx4]\n",
        "    yte = yva_my[idx4]\n",
        "    vte = vva_my[idx4]\n",
        "\n",
        "    model1.fit(Xtr,ytr,batch_size=512, callbacks=[callback], verbose=2, epochs=400, validation_data=(Xva,yva))\n",
        "\n",
        "    Plain_preds = model1.predict(Xte)\n",
        "    deviance = mean_poisson_deviance(yte, Plain_preds)\n",
        "    error = mean_squared_error(yte, Plain_preds)\n",
        "\n",
        "    BS.loc[i, ['PD']] = deviance\n",
        "    BS.loc[i, ['Error']] = error\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "print((end - start)/60.0, \"min elapsed.\")\n",
        "\n",
        "print(BS.mean())\n",
        "print(BS.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7841a6e5",
      "metadata": {
        "id": "7841a6e5"
      },
      "source": [
        "The first transfer model has a MPD of 0.4554(0.0299) with a MSE of 0.0991(0.0120) which doesn't improve the performance compared to the baseline.\n",
        "\n",
        "We move on to freezing less layers from the transferred source model. This will allow us to use more layers to adapt the model to the target data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab0bf279",
      "metadata": {
        "id": "ab0bf279"
      },
      "source": [
        "### Transferred model with two unfrozen layers (Transfer 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81eed03d",
      "metadata": {
        "id": "81eed03d"
      },
      "source": [
        "To check the effect of freezing fewer layers from the transferred model, we run additional experiments that uses more layers for training on the target data.\n",
        "\n",
        "We load the chosen 'model1', remove 2 dense layers including the output layer, and freeze the remaining layers. Then we add 2 new dense layers which also includes a new output layer that is trainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfe8c260",
      "metadata": {
        "id": "cfe8c260"
      },
      "outputs": [],
      "source": [
        "model2 = Sequential()\n",
        "\n",
        "for layer in model.layers[:-2]:\n",
        "    model2.add(layer)\n",
        "for layer in model2.layers:\n",
        "    layer.trainable=False\n",
        "\n",
        "model2.add(Dense(1, activation='linear',name='dense_3'))\n",
        "model2.add(Dense(1, activation='exponential',name='dense_4'))\n",
        "model2.compile(optimizer=Nadam(0.004), loss='poisson')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a406e9c",
      "metadata": {
        "id": "7a406e9c"
      },
      "source": [
        "We train the new model on the target data and record the metrics.\n",
        "\n",
        "The second transfer model has a MPD of 0.4523(0.0295) with a MSE of 0.0988(0.0120) which shows an improvement compared to the first transfer model. But this is still worse than the baseline model without any transfer learning.\n",
        "\n",
        "We experiment once more with freezing less number of dense layers from the transferred source model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74f05545",
      "metadata": {
        "id": "74f05545"
      },
      "source": [
        "### Transferred model with three unfrozen dense layers (Transfer 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98cd4faf",
      "metadata": {
        "id": "98cd4faf"
      },
      "source": [
        "We load the chosen 'model1' and remove 3 dense layers including the dropout, batchnormalization,activation, and output layer. Then the remaining layers are frozen. Finally, 3 new dense layers, dropout, activation, and a new output layer is added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43018d7e",
      "metadata": {
        "id": "43018d7e"
      },
      "outputs": [],
      "source": [
        "model3 = Sequential()\n",
        "\n",
        "for layer in model.layers[:-6]:\n",
        "    model3.add(layer)\n",
        "for layer in model3.layers:\n",
        "    layer.trainable=False\n",
        "\n",
        "model3.add(Dense(14,kernel_initializer = 'GlorotNormal',name='dense_2'))\n",
        "model3.add(BatchNormalization(name='batch_normalization_2'))\n",
        "model3.add(Dropout(0.1,name='dropout_2'))\n",
        "model3.add(Activation('tanh',name='activation_2'))\n",
        "model3.add(Dense(1, activation='linear',name='dense_3'))\n",
        "model3.add(Dense(1, activation='exponential',name='dense_4'))\n",
        "model3.compile(optimizer=Nadam(0.004), loss='poisson')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "531095d9",
      "metadata": {
        "id": "531095d9"
      },
      "source": [
        "We train the new model on the target data and record the metrics.\n",
        "\n",
        "The third transfer model has a MPD of 0.4414(0.0315) with a MSE of 0.0975(0.0128) which shows an improvement compared to the second transfer model and is also better than the baseline model.\n",
        "\n",
        "In summary, parameter-based transfer learning in the inductive setting does show improvement in the prediction performance.\n",
        "One possible reason for the poor performance of the first and second transfer models is that the source task (i.e., predicting auto claims due to robbery and fire risks) is not that close to the target task (i.e., auto claims prediction due to injury). Therefore, it would require unfreezing more layers to adapt the source task to the target task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51cb6a44",
      "metadata": {
        "id": "51cb6a44"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}