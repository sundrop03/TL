{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eeccc18",
   "metadata": {},
   "source": [
    "# Transductive transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148af369",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00772ed8",
   "metadata": {},
   "source": [
    "This notebook demonstrates the applications of transductive transfer learning discussed in the paper [\"Transfer Learning in the Actuarial Domain\"]().\n",
    "\n",
    "The python package [ADAPT](https://github.com/adapt-python/adapt) is used for the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45e94a0",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e7bb2c",
   "metadata": {},
   "source": [
    "For transfer learning, we need a source dataset to learn from and a target dataset to transfer the learnings to.\n",
    "\n",
    "Therefore, two datasets are used for the application.\n",
    "\n",
    "For the source dataset, we use the Australian automobile claims data that can be accessed from the [CASdatasets](https://github.com/dutangc/CASdatasets). And the target dataset is a Singapore automobile claims data that is also available in the CASdatasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f74f12",
   "metadata": {},
   "source": [
    "We start with loading the necessary packages that include various metrics and loss functions used in constructing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013e2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_poisson_deviance\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.backend import exp\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Reshape, Dense, Activation, Flatten, Concatenate, Embedding, BatchNormalization, Dropout, Add\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, SGD, Adamax, Adagrad\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd8453",
   "metadata": {},
   "source": [
    "The original source data has 11 features available which are veh_value, exposure, clm, numclaims, claimcst0, veh_body, veh_age, gender, area, agecat, X_OBSTAT_.\n",
    "\n",
    "We use 6 features which are gender, exposure, veh_body, veh_age, agecat, and numclaims. These features are pre-processed to match the features of our target data. Gender is changed to Female as a binary data of 0 and 1. No change is made for the Exposure feature. Veh_body denotes the vehicle body categorized as Bus, Convertible, Coupe, Hatchback, Hardtop, Minibus, Motorized caravan, Panel van, Roadster, Sedan, Station wagon, Truck, and Utility. Based on the veh_body, we create a feature Veh_type and categorize all vehicles into A(auto), O(others), and T(truck). The Veh_age is grouped from 1 to 4 with 1 being the youngest and 4 representing the oldest. Age_Cat is based on the agecat and categorizes the policyholder into 5 age groups where 1 is the youngest and 5 is the oldest age group. Numclaims represents the number of claims and is renamed to N_Claims.\n",
    "\n",
    "Now we have 6 final features in our source data which are Female, Exposure, Veh_type, Veh_age, Age_Cat, N_Claims."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c338d",
   "metadata": {},
   "source": [
    "Let's load our source data and check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc80ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mysource= pd.read_excel(\"Source_datav1.xlsx\")\n",
    "print(mysource)\n",
    "mysource.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc156772",
   "metadata": {},
   "source": [
    "All features except Exposure are categorical variables that need to be encoded to be used for machine learning methods.\n",
    "\n",
    "Vehicle type is a categorical variable, but without ordinal characteristics, so we apply one-hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b554ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process source data for use\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder_mysource = pd.DataFrame(encoder.fit_transform(mysource[['Veh_type']]).toarray())\n",
    "mysource1 = mysource.join(encoder_mysource)\n",
    "mysource1.drop('Veh_type', axis=1, inplace=True)\n",
    "mysource1.columns = ['Female', 'Exposure', 'Veh_age', 'Age_Cat', 'N_Claims', 'Auto', 'Other', 'Truck']\n",
    "print(mysource1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f96dad",
   "metadata": {},
   "source": [
    "The original target data has 15 features available from which we use information on gender, exposure, vehicle age, vehicle type, driver age, and number of claims. The 6 final features in our target data are the same as the source data.\n",
    "\n",
    "Now let's load our target data and check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88825c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytarget= pd.read_excel(\"Target_datav1.xlsx\")\n",
    "print(mytarget)\n",
    "mytarget.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec76186",
   "metadata": {},
   "source": [
    "The same encoding is applied to the target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de1e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing for target data\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder_mytarget = pd.DataFrame(encoder.fit_transform(mytarget[['Veh_type']]).toarray())\n",
    "final_target = mytarget.join(encoder_mytarget)\n",
    "final_target.drop('Veh_type', axis=1, inplace=True)\n",
    "final_target.columns = ['Female', 'Exposure', 'Veh_age', 'Age_Cat', 'N_Claims', 'Auto', 'Other', 'Truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe6c1ee",
   "metadata": {},
   "source": [
    "## Setting seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc96f2b",
   "metadata": {},
   "source": [
    "We can set seeds for reproducibility, but exact reproduction of results is hard to achieve due to randomness in the sampling, cross-validation, dropout layers, and methods. This doesn't change the overall findings from the results.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 10579\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_val)\n",
    "import random\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "tf.random.set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf737918",
   "metadata": {},
   "source": [
    "## Defining the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dbe3fe",
   "metadata": {},
   "source": [
    "We define the structure of our neural network that will be used throughout the applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b0bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define NN model\n",
    "\n",
    "def NN_model():\n",
    "    input_init = Input(shape=(6,))\n",
    "    \n",
    "    hidden1 = Dense(12)(input_init) #Hidden 1\n",
    "    batch1 = BatchNormalization()(hidden1) #BatchNormalization1\n",
    "    drop1 = Dropout(0.1)(batch1) #Dropout1\n",
    "    act1 = Activation('tanh')(drop1)\n",
    "\n",
    "    hidden2 = Dense(9)(act1) #Hidden 2\n",
    "    batch2 = BatchNormalization()(hidden2) #BatchNormalization2\n",
    "    drop2 = Dropout(0.1)(batch2) #Dropout2\n",
    "    act2 = Activation('tanh')(drop2)\n",
    "\n",
    "    hidden3 = Dense(6)(act2) #Hidden 3\n",
    "    batch3 = BatchNormalization()(hidden3) #BatchNormalization3\n",
    "    drop3 = Dropout(0.1)(batch3) #Dropout3\n",
    "    act3 = Activation('tanh')(drop3)\n",
    "    lin = Dense(1, activation='linear')(act3)\n",
    "    \n",
    "    output = Dense(1, activation='exponential',trainable = False)(lin) #Output\n",
    "    model = Model(inputs=input_init,outputs=output)\n",
    "    model.compile(optimizer=Nadam(0.004), loss='poisson')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a396b6",
   "metadata": {},
   "source": [
    "We define the model using a functional api to allow for multiple inputs to be fed at any stage of the network. The same model can also be defined as a sequential model.\n",
    "\n",
    "The network has one input layer with 6 neurons that take the 6 features as input. These are passed through three Dense(hidden) layers with tanh activation functions that have 12, 9, and 6 neurons respectively. Then the output is combined and passed through a dense layer with 1 neuron and a linear activation function. Finally, the results are combined at the output layer with an exponential activation function and the loss function is set to 'poisson' since our outcome is the predicted claim frequency. \n",
    "\n",
    "The choice of optimizer was determined by experiments. We have considered a sgd (Stochastic Gradient Descent) with learning rate of 0.1 and momentum of 0.9 as a possible alternative. With dropouts applied to multiple layers, increasing the learning rate and momentum to a higher level provided reasonable results. But this also resulted in very large weights that showed poor performance in terms of predictions. The final choice is Nadam which is an Adam optimizer with Nesterov momentum. The learning rate is set to 0.004 which depends on the batch size used for the training.\n",
    "\n",
    "To control for overfitting, we apply regularization such as batch normalization and dropout to the first three dense layers. There are different opinions regarding the order of applying batch normalization and dropout suggesting dropout coming first. Also, it is quite common to use higher probabilities (e.g., 80\\%) used in dropout for input layers and relatively lower probabilities (e.g., 50\\%) for other layers. But these differ by application and the proposed order and hyperparameters are determined through experiments.\n",
    "\n",
    "Having a large dropout probability works relatively well when training a neural network on a single dataset. But for the application of transfer learning, small dropout probablility worked better. Therefore, we set it to a constant 0.1 to retain most of the output from all layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de2e931",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce54d5b2",
   "metadata": {},
   "source": [
    "We are interested in the performance difference between models with and without transfer learning. Therefore, we run a baseline model that only learns from the target data.\n",
    "\n",
    "The target data is split into 60\\% train, 25\\% validation, and 15\\% test data. The vehicle age group and policyholder age group are categorical variables that are ordinal at the same time. These two features are label encoded and assigned a numerical value to preserve the order of the groups.\n",
    "\n",
    "The outcome of interest \"N_Claims\" should be considered with its corresponding \"Exposure\". In the model, we take this into account by creating bootstrap samples distributed according to the the \"Exposure\" feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1dcbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback =tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0,patience=20,verbose=0,mode='auto',baseline=None,restore_best_weights=True)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "N = 1000\n",
    "BS = pd.DataFrame(columns=['PD','Error'])\n",
    "for i in range(N):\n",
    "    if (i % 200 == 0):\n",
    "        print(\"Iteration\", i)\n",
    "    \n",
    "    #scaling target variables\n",
    "    scalerBA = MinMaxScaler()\n",
    "    final_target[['Age_Cat','Veh_age']] = scalerBA.fit_transform(final_target[['Age_Cat','Veh_age']])\n",
    "    Xmy = final_target.drop(columns = ['N_Claims','Exposure']).values\n",
    "    ymy = final_target['N_Claims'].values\n",
    "    vmy= final_target['Exposure'].values\n",
    "    \n",
    "    \n",
    "    #Creating bootstrap samples taking into account the sample weights based on exposure\n",
    "    idx_boot = np.random.choice(np.arange(len(Xmy)), len(Xmy), replace=True, p=vmy/vmy.sum())\n",
    "    Xmy_bs = Xmy[idx_boot]\n",
    "    ymy_bs = ymy[idx_boot]\n",
    "    vmy_bs = vmy[idx_boot]\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    idx1 = rng.choice(np.arange(len(Xmy_bs)), round(len(Xmy_bs)*0.6), replace=False)\n",
    "    idx2 = np.delete(np.arange(len(Xmy_bs)),idx1)\n",
    "\n",
    "    #training set\n",
    "    Xtr = Xmy_bs[idx1]\n",
    "    ytr = ymy_bs[idx1]\n",
    "    vtr = vmy_bs[idx1]\n",
    "    \n",
    "    Xva_my = Xmy_bs[idx2]\n",
    "    yva_my = ymy_bs[idx2]\n",
    "    vva_my = vmy_bs[idx2]\n",
    "\n",
    "    rng1 = np.random.default_rng()\n",
    "    idx3 = rng1.choice(np.arange(len(Xva_my)), round(len(Xva_my)*0.625), replace=False)\n",
    "    idx4 = np.delete(np.arange(len(Xva_my)),idx3)\n",
    "    \n",
    "    #validation set\n",
    "    Xva = Xva_my[idx3]\n",
    "    yva = yva_my[idx3]\n",
    "    vva = vva_my[idx3]\n",
    "    \n",
    "    #test set\n",
    "    Xte = Xva_my[idx4]\n",
    "    yte = yva_my[idx4]\n",
    "    vte = vva_my[idx4]\n",
    "\n",
    "    \n",
    "    y_va = yva.reshape((len(yva),1))\n",
    "    v_te = vte.reshape((len(vte),1))\n",
    "    y_te = yte.reshape((len(yte),1))\n",
    "    y_sam = ytr.reshape((len(ytr),1))\n",
    "    v_sam = vtr.reshape((len(vtr),1))\n",
    "    \n",
    "    \n",
    "    model_TL = NN_model()\n",
    "    model_TL.fit(Xtr,y_sam,batch_size=512, callbacks=[callback], verbose=2, epochs=300, validation_data=(Xva,y_va))\n",
    "    \n",
    "    TL_preds = model_TL.predict(Xte)\n",
    "    \n",
    "    deviance = mean_poisson_deviance(y_te, TL_preds)\n",
    "    error = mean_squared_error(y_te, TL_preds)\n",
    "    \n",
    "    BS.loc[i, ['PD']] = deviance\n",
    "    BS.loc[i, ['Error']] = error\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/60.0, \"min elapsed.\")\n",
    "\n",
    "print(BS.mean())\n",
    "print(BS.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de4f55",
   "metadata": {},
   "source": [
    "We set the model to have 1,000 iterations to produce 1,000 sets of results. For each iteration, we calculate the MPD(Mean Poisson Deviance) and MSE(Mean Squared Error) to determine the performance of the model. If the predictions are closer to the true outcome, then the deviation decreases, which indicates an improvement in the predictions. Our problem is a Poisson regression problem, so using MPD with a traditional regression metric MSE will be a reasonable choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86803971",
   "metadata": {},
   "source": [
    "The batch size is determined through a balance between runtime and gain in performance. Usually a small batch size of 32 or 64 with a learning rate of 0.001 provides a good prediction, but it greatly increases the runtime. We increased the batch size to 512 and the learning rate of the optimizer was increased to 0.004, which is by a factor of $\\sqrt{16}$. Increasing the learning rate by a factor of the increase in batch size resulted in less runtime with similar predictive performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b33731",
   "metadata": {},
   "source": [
    "The baseline model has a MPD of 0.4610(0.0731) with a MSE of 0.1027(0.0316). Values in the parentheses are standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3903f36",
   "metadata": {},
   "source": [
    "Now we have the baseline model! Let's move on to apply transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0176c6e",
   "metadata": {},
   "source": [
    "## Transfer learning methods with no target labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5500636f",
   "metadata": {},
   "source": [
    "We experiment with instance-based and feature-based methods that don't require target labels for the learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a554b3c",
   "metadata": {},
   "source": [
    "### Instance-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd6cc7c",
   "metadata": {},
   "source": [
    "Let's start with instance-based methods. The instance-based approach is aiming to minimize the marginal distribution differences between the source data and the target data by reweighting the instances. We experiment with KMM and KLIEP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa27d2",
   "metadata": {},
   "source": [
    "### KMM(Kernel Mean Matching)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70794554",
   "metadata": {},
   "source": [
    "KMM minimizes the difference in means between the input of source and target instances in a reproducing kernel Hilbert space (RKHS) by reweighting the source instances. Matching the mean is equivalent to minimizing the discrepancy of the marginal distributions between the source and target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c208aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adapt\n",
    "from adapt.instance_based import KMM\n",
    "callback =tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0,patience=20,verbose=0,mode='auto',baseline=None,restore_best_weights=True)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "N = 1000\n",
    "BS = pd.DataFrame(columns=['PD','Error'])\n",
    "for i in range(N):\n",
    "    if (i % 200 == 0):\n",
    "        print(\"Iteration\", i)\n",
    "\n",
    "    #scaling source data\n",
    "    scalerKMMA = MinMaxScaler()\n",
    "    mysource1[['Age_Cat','Veh_age']] = scalerKMMA.fit_transform(mysource1[['Age_Cat','Veh_age']])\n",
    "    Xmy = mysource1.drop(columns = ['N_Claims','Exposure']).values\n",
    "    ymy = mysource1['N_Claims'].values\n",
    "    vmy= mysource1['Exposure'].values\n",
    "    \n",
    "    #Creating bootstrap samples taking into account the sample weights based on exposure\n",
    "    idx_boot = np.random.choice(np.arange(len(Xmy)), len(Xmy), replace=True, p=vmy/vmy.sum())\n",
    "    Xmy_bs = Xmy[idx_boot]\n",
    "    ymy_bs = ymy[idx_boot]\n",
    "    vmy_bs = vmy[idx_boot]\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    idx1 = rng.choice(np.arange(len(Xmy_bs)), round(len(Xmy_bs)*0.8), replace=False)\n",
    "    idx2 = np.delete(np.arange(len(Xmy_bs)),idx1)\n",
    "    \n",
    "    #training set (source)\n",
    "    Xs = Xmy_bs[idx1]\n",
    "    ys = ymy_bs[idx1]\n",
    "    vs = vmy_bs[idx1]\n",
    "    \n",
    "    #validation set (source)\n",
    "    Xva = Xmy_bs[idx2]\n",
    "    yva = ymy_bs[idx2]\n",
    "    vva = vmy_bs[idx2]        \n",
    "        \n",
    "    #split training and testing data within target\n",
    "    train_target = final_target.sample(frac=0.6, random_state=None)\n",
    "    test_target = final_target.drop(train_target.index)\n",
    "\n",
    "    #scale target data\n",
    "    scalerKMM1 = MinMaxScaler()\n",
    "    train_target[['Age_Cat','Veh_age']] = scalerKMM1.fit_transform(train_target[['Age_Cat','Veh_age']])\n",
    "    Xtr = train_target.drop(columns = ['N_Claims','Exposure']).values\n",
    "    ytr = train_target['N_Claims'].values\n",
    "    vtr = train_target['Exposure'].values\n",
    "    dump(scalerKMM1, open('scalerKMM1.pkl', 'wb'))\n",
    "\n",
    "    scalerKMM1 = load(open('scalerKMM1.pkl', 'rb'))\n",
    "    test_target[['Age_Cat','Veh_age']] = scalerKMM1.transform(test_target[['Age_Cat','Veh_age']])\n",
    "    Xte = test_target.drop(columns = ['N_Claims','Exposure']).values\n",
    "    yte = test_target['N_Claims'].values\n",
    "    vte = test_target['Exposure'].values\n",
    "    \n",
    "    #training set (target)\n",
    "    idx = np.random.choice(np.arange(len(Xtr)), len(Xtr), replace=True, p=vtr/vtr.sum())\n",
    "    x_sample = Xtr[idx]\n",
    "    y_sample = ytr[idx]\n",
    "    v_sample = vtr[idx]\n",
    "\n",
    "    y_s = ys.reshape((len(ys),1))\n",
    "    v_s = vs.reshape((len(vs),1))\n",
    "    y_va = yva.reshape((len(yva),1))\n",
    "    v_te = vte.reshape((len(vte),1))\n",
    "    y_te = yte.reshape((len(yte),1))\n",
    "    y_sam = y_sample.reshape((len(y_sample),1))\n",
    "    v_sam = v_sample.reshape((len(v_sample),1))\n",
    "    \n",
    "    \n",
    "    model_TL = KMM(NN_model(), Xt=x_sample, kernel=\"rbf\", gamma=3)\n",
    "    model_TL.fit(X=Xs, y=y_s, Xt=x_sample, callbacks=[callback], validation_data=(Xva,y_va), epochs=300, batch_size=512, verbose=2)\n",
    "    \n",
    "    TL_preds = model_TL.predict(Xte)\n",
    "    \n",
    "    deviance = mean_poisson_deviance(y_te, TL_preds)\n",
    "    error = mean_squared_error(y_te, TL_preds)\n",
    "    \n",
    "    BS.loc[i, ['PD']] = deviance\n",
    "    BS.loc[i, ['Error']] = error\n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/60.0, \"min elapsed.\")\n",
    "\n",
    "print(BS.mean())\n",
    "print(BS.std())\n",
    "\n",
    "writer = pd.ExcelWriter('KMM.xlsx', engine='xlsxwriter')\n",
    "BS.to_excel(writer, sheet_name='welcome', index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b413b",
   "metadata": {},
   "source": [
    "The value for parameter 'gamma' is determined by cross-validation.\n",
    "The KMM model has a MPD of 0.4113(0.0237) with a MSE of 0.0816(0.0078) which is an improvement compared to the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efecf45",
   "metadata": {},
   "source": [
    "### KLIEP(Kullback–Leibler Importance Estimation Procedure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ca079",
   "metadata": {},
   "source": [
    "KLIEP minimizes the Kullback-Leibler(KL) discrepancy between the source and target marginal distribution. This is achieved by reweighting the source instances to minimize the KL divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e365e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adapt\n",
    "from adapt.instance_based import KLIEP\n",
    "\n",
    "model_TL = KLIEP(NN_model(), Xt=x_sample, kernel=\"rbf\", gamma=1)\n",
    "model_TL.fit(Xs, y_s, Xt=x_sample, callbacks=[callback], validation_data=(Xva,y_va), epochs=300, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce88566",
   "metadata": {},
   "source": [
    "The KLIEP model follows the same source training, source validation, target training, and target test data split used for the KMM model. These two methods require the source input, source label, and the target input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7841a6e5",
   "metadata": {},
   "source": [
    "The value of parameter 'gamma' is determined by cross-validation. Providing a list of potential 'gamma' values in the fit function would automatically call a 5 fold cross validation to determine the best 'gamma'. The KLIEP model has a MPD of 0.4152(0.0209) with a MSE of 0.0802(0.0059) which improves the performance compared to the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0bf279",
   "metadata": {},
   "source": [
    "### Feature-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eed03d",
   "metadata": {},
   "source": [
    "Let's consider feature-based methods. The idea of feature-based approaches is to minimize the differences between the source and target by transforming the features. We experiment with CORAL and Deep CORAL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b360651",
   "metadata": {},
   "source": [
    "### CORAL(Correlation Alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f7d134",
   "metadata": {},
   "source": [
    "CORAL transforms the source features to minimize the discrepancy between the correlation matrices of the transformed source\n",
    "and the non-transformed target. A linear transformation is applied to the covariance matrix of the source features which minimizes the distance between the transformed source covariance and the non-transformed target covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe8c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adapt\n",
    "from adapt.feature_based import CORAL\n",
    "\n",
    "model_TL = CORAL(NN_model(), Xt=x_sample)\n",
    "model_TL.fit(X=Xs, y=y_s, Xt=x_sample, callbacks=[callback], validation_data=(Xva,y_va), epochs=300, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a406e9c",
   "metadata": {},
   "source": [
    "CORAL requires a parameter 'lambda_' which is the regularization parameter that affects the level of adaptation. In the\n",
    "experiment, we used the default lambda of 0.00001 to allow a higher level of adaptation between the covariances of the source and target inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c53ecb",
   "metadata": {},
   "source": [
    "The CORAL model has a MPD of 0.4032(0.0427) with a MSE of 0.0789(0.0213) which is better than the instance-based methods KMM and KLIEP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f05545",
   "metadata": {},
   "source": [
    "### Deep CORAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cd4faf",
   "metadata": {},
   "source": [
    "Deep CORAL is an extension of CORAL where the learning is done in two parts. First, an encoder network is trained to learn new feature representations such that the correlation matrices of the source and target data are close. Second, these new feature representations are used for the source data to enable a task network to learn a model that predicts the frequency of claims.\n",
    "\n",
    "Now the total loss to minize can be thought in two parts. A CORAL loss that comes from encoder network, which is the difference in covariance matrices between the source and target input. And a task loss to minimize for our prediction problem using the task network.\n",
    "\n",
    "We set the encoder model as a shallow network with 6 neurons and tanh activation. The task model is the same as the baseline neural network model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43018d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define NN encoder model\n",
    "\n",
    "def NN_model0(input_shape=(6,)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, activation='tanh')) #Hidden 3\n",
    "    model.compile(optimizer=Nadam(0.004), loss='poisson')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5c5d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define NN task model\n",
    "\n",
    "def NN_model1():\n",
    "    input_init = Input(shape=(6,))\n",
    "    \n",
    "    hidden1 = Dense(12)(input_init) #Hidden 1\n",
    "    batch1 = BatchNormalization()(hidden1) #BatchNormalization1\n",
    "    drop1 = Dropout(0.1)(batch1) #Dropout1\n",
    "    act1 = Activation('tanh')(drop1)\n",
    "\n",
    "    hidden2 = Dense(9)(act1) #Hidden 2\n",
    "    batch2 = BatchNormalization()(hidden2) #BatchNormalization2\n",
    "    drop2 = Dropout(0.1)(batch2) #Dropout2\n",
    "    act2 = Activation('tanh')(drop2)\n",
    "\n",
    "    hidden3 = Dense(6)(act2) #Hidden 3\n",
    "    batch3 = BatchNormalization()(hidden3) #BatchNormalization3\n",
    "    drop3 = Dropout(0.1)(batch3) #Dropout3\n",
    "    act3 = Activation('tanh')(drop3)\n",
    "    lin = Dense(1, activation='linear')(act3)\n",
    "    \n",
    "    output = Dense(1, activation='exponential',trainable = False)(lin) #Output\n",
    "    model = Model(inputs=input_init,outputs=output)\n",
    "    model.compile(optimizer=Nadam(0.004), loss='poisson')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08947c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adapt\n",
    "from adapt.feature_based import DeepCORAL\n",
    "modelEn = NN_model0()\n",
    "modelTk = NN_model1()\n",
    "\n",
    "model_TL = DeepCORAL(encoder=modelEn,task=modelTk, Xt=x_sample, lambda_= 0.8)\n",
    "model_TL.fit(X=Xs, y=y_s, Xt=x_sample,callbacks=[callback], validation_data=(Xva,y_va), epochs=300, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531095d9",
   "metadata": {},
   "source": [
    "$\\mathcal{L}_{total}=\\mathcal{L}_{task}+\\lambda\\mathcal{L}_{coral}$\n",
    "\n",
    "The parameter 'lambda_' plays a role to balance the CORAL loss and the task loss, which enables the learned model using source data with new feature representations to make good predictions on the target data. A higher value of 'lambda_' heavily penalizes the difference in covariance matrices between the source and target. We set the value of 'lambda_' to 0.8 which was determined by cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d06250",
   "metadata": {},
   "source": [
    "The Deep CORAL model has a MPD of 0.4006(0.0135) with a MSE of 0.0778(0.0050) which is better than the CORAL model.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa011d2a",
   "metadata": {},
   "source": [
    "## Transfer learning models with target labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202d5d0",
   "metadata": {},
   "source": [
    "We experiment with instance-based, feature-based, and parameter-based methods that require target labels for the learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcd4bd1",
   "metadata": {},
   "source": [
    "### Instance-based method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af0e792",
   "metadata": {},
   "source": [
    "For the instance-based approach, we examine the Transfer AdaBoost for regression (“TrAdaBoostR2”) which is a boosting-based approach that uses reverse boosting to update the weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6029bd",
   "metadata": {},
   "source": [
    "### TrAdaBoostR2(Transfer AdaBoost for Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288858d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adapt\n",
    "from adapt.instance_based import TrAdaBoostR2\n",
    "\n",
    "model_TL = TrAdaBoostR2(NN_model(), n_estimators=10, lr=0.00001, Xt=x_sample, yt=y_sam)\n",
    "model_TL.fit(Xs, y_s, Xt=x_sample,yt=y_sam, callbacks=[callback], validation_data=(Xva,y_va), epochs=300, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb64ea3",
   "metadata": {},
   "source": [
    "TrAdaBoostR2 is a boosting-based method that needs to have a defined the number of boosting iterations. This is the parameter 'n_estimators' which is by default 10. The learning rate 'lr' is set to 0.00001, which determines how fast the weights are updated. In the fit function of the code, we can see that the target labels 'y_sam' is used at the training stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadd4db5",
   "metadata": {},
   "source": [
    "The TrAdaBoostR2 model has a MPD of 0.3950(0.0121) with a MSE of 0.0776(0.0049) which is better than all the models that didn't have access to target labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5660e5",
   "metadata": {},
   "source": [
    "### Feature-based method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df89aac0",
   "metadata": {},
   "source": [
    "For the feature-based approach, we implement the Feature Augmentation (“FA”) method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a77b4bc",
   "metadata": {},
   "source": [
    "### FA(Feature Augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f87ec",
   "metadata": {},
   "source": [
    "Feature Augmentation is done by adding null vectors to both the source and target vectors which results in all the features having three components.\n",
    "The source feature Xs becomes (Xs, 0, Xs) and the target feature becomes (0, Xt, Xt).\n",
    "The transformed source and target data are combined and used as the training data to learn a model which is then used to predict the outcome of the test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953d6b7",
   "metadata": {},
   "source": [
    "We have 6 features for the source and target, so the transformed source and target will have 18 features. Therefore, we modify the neural network to take into account the change in input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3046690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define NN model\n",
    "\n",
    "def NN_model():\n",
    "    input_init = Input(shape=(18,))\n",
    "    \n",
    "    hidden1 = Dense(36)(input_init) #Hidden 1\n",
    "    batch1 = BatchNormalization()(hidden1) #BatchNormalization1\n",
    "    drop1 = Dropout(0.1)(batch1) #Dropout1\n",
    "    act1 = Activation('tanh')(drop1)\n",
    "\n",
    "    hidden2 = Dense(27)(act1) #Hidden 2\n",
    "    batch2 = BatchNormalization()(hidden2) #BatchNormalization2\n",
    "    drop2 = Dropout(0.1)(batch2) #Dropout2\n",
    "    act2 = Activation('tanh')(drop2)\n",
    "\n",
    "    hidden3 = Dense(18)(act2) #Hidden 3\n",
    "    batch3 = BatchNormalization()(hidden3) #BatchNormalization3\n",
    "    drop3 = Dropout(0.1)(batch3) #Dropout3\n",
    "    act3 = Activation('tanh')(drop3)\n",
    "    lin = Dense(1, activation='linear')(act3)\n",
    "    \n",
    "    output = Dense(1, activation='exponential',trainable = False)(lin) #Output\n",
    "    model = Model(inputs=input_init,outputs=output)\n",
    "    model.compile(optimizer=Nadam(0.004), loss='poisson')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5605f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "N = 1000\n",
    "BS = pd.DataFrame(columns=['PD','Error'])\n",
    "for i in range(N):\n",
    "    if (i % 200 == 0):\n",
    "        print(\"Iteration\", i)\n",
    "\n",
    "    #scaling source variables\n",
    "    \n",
    "    scalerFA = MinMaxScaler()\n",
    "    mysource1[['Age_Cat','Veh_age']] = scalerFA.fit_transform(mysource1[['Age_Cat','Veh_age']])\n",
    "    Xmy = mysource1.drop(columns = ['N_Claims','Exposure']).values\n",
    "    ymy = mysource1['N_Claims'].values\n",
    "    vmy= mysource1['Exposure'].values\n",
    "\n",
    "    idx_boot = np.random.choice(np.arange(len(Xmy)), len(Xmy), replace=True, p=vmy/vmy.sum())\n",
    "    Xmy_bs = Xmy[idx_boot]\n",
    "    ymy_bs = ymy[idx_boot]\n",
    "    vmy_bs = vmy[idx_boot]\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    idx1 = rng.choice(np.arange(len(Xmy_bs)), round(len(Xmy_bs)*0.8), replace=False)\n",
    "    idx2 = np.delete(np.arange(len(Xmy_bs)),idx1)\n",
    "\n",
    "    Xs = Xmy_bs[idx1]\n",
    "    ys = ymy_bs[idx1]\n",
    "    vs = vmy_bs[idx1]\n",
    "\n",
    "    Xva = Xmy_bs[idx2]\n",
    "    yva = ymy_bs[idx2]\n",
    "    vva = vmy_bs[idx2]\n",
    "        \n",
    "    #split training and testing data within target\n",
    "    train_target = final_target.sample(frac=0.6, random_state=None)\n",
    "    test_target = final_target.drop(train_target.index)\n",
    "\n",
    "    #scale target data separately\n",
    "    scalerFA1 = MinMaxScaler()\n",
    "    train_target[['Age_Cat','Veh_age']] = scalerFA1.fit_transform(train_target[['Age_Cat','Veh_age']])\n",
    "    Xtr = train_target.drop(columns = ['N_Claims','Exposure']).values\n",
    "    ytr = train_target['N_Claims'].values\n",
    "    vtr = train_target['Exposure'].values\n",
    "    dump(scalerFA1, open('scalerFA1.pkl', 'wb'))\n",
    "\n",
    "    scalerFA1 = load(open('scalerFA1.pkl', 'rb'))\n",
    "    test_target[['Age_Cat','Veh_age']] = scalerFA1.transform(test_target[['Age_Cat','Veh_age']])\n",
    "    Xte = test_target.drop(columns = ['N_Claims','Exposure']).values\n",
    "    yte = test_target['N_Claims'].values\n",
    "    vte = test_target['Exposure'].values\n",
    "        \n",
    "    idx = np.random.choice(np.arange(len(Xtr)), len(Xtr), replace=True, p=vtr/vtr.sum())\n",
    "    x_sample = Xtr[idx]\n",
    "    y_sample = ytr[idx]\n",
    "    v_sample = vtr[idx]\n",
    "    \n",
    "    y_s = ys.reshape((len(ys),1))\n",
    "    v_s = vs.reshape((len(vs),1))\n",
    "    y_va = yva.reshape((len(yva),1))\n",
    "    v_te = vte.reshape((len(vte),1))\n",
    "    y_te = yte.reshape((len(yte),1))\n",
    "    y_sam = y_sample.reshape((len(y_sample),1))\n",
    "    v_sam = v_sample.reshape((len(v_sample),1))\n",
    "    \n",
    "    #transform source and target data\n",
    "    Xs_emb = np.concatenate((Xs,np.zeros((len(Xs), Xs.shape[-1])),Xs),axis=-1)\n",
    "    Xtr_emb = np.concatenate((np.zeros((len(x_sample), x_sample.shape[-1])),x_sample,x_sample),axis=-1)\n",
    "    Xva_emb = np.concatenate((Xva,np.zeros((len(Xva), Xva.shape[-1])),Xva),axis=-1)\n",
    "    Xte_emb = np.concatenate((np.zeros((len(Xte), Xte.shape[-1])),Xte,Xte),axis=-1)\n",
    "    \n",
    "    #Augment the transformed source and target data\n",
    "    X_aug = np.concatenate((Xs_emb, Xtr_emb))\n",
    "    y_aug = np.concatenate((y_s, y_sam))\n",
    "    v_aug = np.concatenate((v_s, v_sam))\n",
    "    \n",
    "    rng1 = np.random.default_rng()\n",
    "    idx3 = rng1.choice(np.arange(len(X_aug)), round(len(X_aug)*0.8), replace=False)\n",
    "    idx4 = np.delete(np.arange(len(X_aug)),idx3)\n",
    "\n",
    "    final_x = X_aug[idx3]\n",
    "    final_y = y_aug[idx3]\n",
    "    final_v = v_aug[idx3]\n",
    "\n",
    "    valid_x = X_aug[idx4]\n",
    "    valid_y = y_aug[idx4]\n",
    "    \n",
    "    cf = sum(y_sam)/sum(v_sam) # claim frequency\n",
    "    Int_ClaimNB = cf*v_te\n",
    "\n",
    "    model_TL = NN_model()\n",
    "    model_TL.fit(final_x, final_y, callbacks=[callback], validation_data=(valid_x,valid_y), epochs=300, batch_size=512, verbose=2)\n",
    "    \n",
    "    TL_preds = model_TL.predict(Xte_emb)\n",
    "    \n",
    "    deviance = mean_poisson_deviance(y_te, TL_preds)\n",
    "    error = mean_squared_error(y_te, TL_preds)\n",
    "    \n",
    "    BS.loc[i, ['PD']] = deviance\n",
    "    BS.loc[i, ['Error']] = error\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/60.0, \"min elapsed.\")\n",
    "\n",
    "print(BS.mean())\n",
    "print(BS.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a195634",
   "metadata": {},
   "source": [
    "The FA model has a MPD of 0.3915(0.0196) with a MSE of 0.0775(0.0065) which is the best of all models in the transductive learning application. FA is a simple feature transformation that can be applied to the source and target data along with applying other transfer learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a70b63",
   "metadata": {},
   "source": [
    "### Paramter-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281092a3",
   "metadata": {},
   "source": [
    "The parameter-based approach is assuming that a good estimator for the target data can be learned by utilizing the parameters of the source estimator. Specifically, transfer learning is done through shared parameters. In the experiment, we consider two approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b551c",
   "metadata": {},
   "source": [
    "### Regularized Transfer Neural Networks(TR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a0c8b5",
   "metadata": {},
   "source": [
    "A neural network model is trained on the source data then its parameters are obtained. Then we use the target data to\n",
    "train a new model using the parameters transferred from the source model. Here the final parameters for the new model are obtained by regularizing the distance between the transferred source parameters and the parameters learned using the target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc31c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adapt\n",
    "from adapt.parameter_based import RegularTransferNN\n",
    "callback =tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0,patience=20,verbose=0,mode='auto',baseline=None,restore_best_weights=True)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "N = 1000\n",
    "BS = pd.DataFrame(columns=['PD','Error','Actual','Predicted','PD_Int'])\n",
    "for i in range(N):\n",
    "    if (i % 200 == 0):\n",
    "        print(\"Iteration\", i)\n",
    "        \n",
    "    #scale source data separately\n",
    "    scalerP1A = MinMaxScaler()\n",
    "    mysource1[['Age_Cat','Veh_age']] = scalerP1A.fit_transform(mysource1[['Age_Cat','Veh_age']])\n",
    "    Xmy = mysource1.drop(columns = ['N_Claims','Exposure']).values\n",
    "    ymy = mysource1['N_Claims'].values\n",
    "    vmy= mysource1['Exposure'].values\n",
    "\n",
    "    idx_boot = np.random.choice(np.arange(len(Xmy)), len(Xmy), replace=True, p=vmy/vmy.sum())\n",
    "    Xmy_bs = Xmy[idx_boot]\n",
    "    ymy_bs = ymy[idx_boot]\n",
    "    vmy_bs = vmy[idx_boot]\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    idx1 = rng.choice(np.arange(len(Xmy_bs)), round(len(Xmy_bs)*0.8), replace=False)\n",
    "    idx2 = np.delete(np.arange(len(Xmy_bs)),idx1)\n",
    "\n",
    "    Xst = Xmy_bs[idx1]\n",
    "    yst = ymy_bs[idx1]\n",
    "    vst = vmy_bs[idx1]\n",
    "\n",
    "    Xsv = Xmy_bs[idx2]\n",
    "    ysv = ymy_bs[idx2]\n",
    "    vsv = vmy_bs[idx2]\n",
    "       \n",
    "    #split training and testing data within target\n",
    "    \n",
    "    scalerP1 = MinMaxScaler()\n",
    "    final_target[['Age_Cat','Veh_age']] = scalerP1.fit_transform(final_target[['Age_Cat','Veh_age']])\n",
    "    Xty = final_target.drop(columns = ['N_Claims','Exposure']).values\n",
    "    yty = final_target['N_Claims'].values\n",
    "    vty= final_target['Exposure'].values\n",
    "\n",
    "    idx_boot = np.random.choice(np.arange(len(Xty)), len(Xty), replace=True, p=vty/vty.sum())\n",
    "    Xty_bs = Xty[idx_boot]\n",
    "    yty_bs = yty[idx_boot]\n",
    "    vty_bs = vty[idx_boot]\n",
    "\n",
    "    rng1 = np.random.default_rng()\n",
    "    idx3 = rng1.choice(np.arange(len(Xty_bs)), round(len(Xty_bs)*0.6), replace=False)\n",
    "    idx4 = np.delete(np.arange(len(Xty_bs)),idx3)\n",
    "\n",
    "    x_sample = Xty_bs[idx3]\n",
    "    y_sample = yty_bs[idx3]\n",
    "    v_sample = vty_bs[idx3]\n",
    "    \n",
    "    Xva_my = Xty_bs[idx4]\n",
    "    yva_my = yty_bs[idx4]\n",
    "    vva_my = vty_bs[idx4]\n",
    "\n",
    "    rng2 = np.random.default_rng()\n",
    "    idx5 = rng2.choice(np.arange(len(Xva_my)), round(len(Xva_my)*0.625), replace=False)\n",
    "    idx6 = np.delete(np.arange(len(Xva_my)),idx5)\n",
    "    \n",
    "    Xva = Xva_my[idx5]\n",
    "    yva = yva_my[idx5]\n",
    "    vva = vva_my[idx5]\n",
    "    \n",
    "    Xte = Xva_my[idx6]\n",
    "    yte = yva_my[idx6]\n",
    "    vte = vva_my[idx6]\n",
    "\n",
    "    #train source model\n",
    "    src_model = RegularTransferNN(task=NN_model(),loss=\"poisson\",lambdas=0)\n",
    "    src_model.fit(Xst, yst, callbacks=[callback], validation_data=(Xsv,ysv), epochs=300, batch_size=512)\n",
    "    \n",
    "    #train regularized model\n",
    "    modelTNN = RegularTransferNN(src_model.task_,loss=\"poisson\", lambdas=0.5)\n",
    "    modelTNN.fit(x_sample, y_sample, callbacks=[callback], validation_data=(Xva,yva), epochs=300, batch_size=512, verbose=2)\n",
    "    \n",
    "    TL_preds = modelTNN.predict(Xte)\n",
    "    deviance = mean_poisson_deviance(yte, TL_preds)\n",
    "    \n",
    "    BS.loc[i, ['PD']] = deviance\n",
    "    BS.loc[i, ['Error']] = error\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/60.0, \"min elapsed.\")\n",
    "\n",
    "print(BS.mean())\n",
    "print(BS.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18c05e",
   "metadata": {},
   "source": [
    "The parameter 'lambdas' for the source model is set to 0 since we are training the model only on the source data. For the regularized model, we set 'lambdas' to 0.5, since we take into account the difference between the source network layer parameter and the target network layer parameters.\n",
    "\n",
    "The Regularized Transfer model has a MPD of 0.4663(0.0445) with a MSE of 0.1018(0.0204) which is worse than the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d835dc1",
   "metadata": {},
   "source": [
    "### Transfer Embeddings(TR_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50570615",
   "metadata": {},
   "source": [
    "Next we consider transferring certain layers from the trained source model to the target model and train the other layers with the target data. In the experiment, we look into transferring the \"embedding\" layers from the source to the target model since our features consist of categorical features only. The code here is from [Entity Embedding Neural Net](https://www.kaggle.com/code/aquatic/entity-embedding-neural-net) and is modified for this application. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da6ff91",
   "metadata": {},
   "source": [
    "The source and target data should be processed differently than other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe320f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import source data\n",
    "mysource= pd.read_excel(\"/home/y/ykim775/PUBLIC_web/Data/Source_datav1.xlsx\")\n",
    "\n",
    "#process source data for use\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "encoder = OrdinalEncoder()\n",
    "mysource[['Veh_type']] = encoder.fit_transform(mysource[['Veh_type']])\n",
    "print(mysource)\n",
    "\n",
    "#import target data\n",
    "mytarget= pd.read_excel(\"/home/y/ykim775/PUBLIC_web/Data/Target_datav1.xlsx\")\n",
    "\n",
    "#process target data for use\n",
    "encoder = OrdinalEncoder()\n",
    "mytarget[['Veh_type']] = encoder.fit_transform(mytarget[['Veh_type']])\n",
    "print(mytarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcea9ef1",
   "metadata": {},
   "source": [
    "We first define the embedding network that takes in the categorical features, creates the embeddings, and trains a model using these embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa46de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define embedding network\n",
    "def build_embedding_network():\n",
    "    \n",
    "    inputs = []\n",
    "    embeddings = []\n",
    "\n",
    "    Female_input = Input(shape=(1,), name='Female')\n",
    "    Fem_emb = Embedding(2, 1, input_length=1, name='Fem_emb')(Female_input)\n",
    "    Flat1 = Reshape(target_shape=(1,))(Fem_emb)\n",
    "    inputs.append(Female_input)\n",
    "    embeddings.append(Flat1)\n",
    "\n",
    "    Veh_type_input = Input(shape=(1,), name='Veh_type')\n",
    "    VehType_emb = Embedding(3, 2, input_length=1, name='VehType_emb')(Veh_type_input)\n",
    "    Flat2 = Reshape(target_shape=(2,))(VehType_emb)\n",
    "    inputs.append(Veh_type_input)\n",
    "    embeddings.append(Flat2)\n",
    "\n",
    "    Veh_age_input = Input(shape=(1,), name='Veh_age')\n",
    "    VehAge_emb = Embedding(4, 2, input_length=1, name='VehAge_emb')(Veh_age_input)\n",
    "    Flat3 = Reshape(target_shape=(2,))(VehAge_emb)\n",
    "    inputs.append(Veh_age_input)\n",
    "    embeddings.append(Flat3)\n",
    "\n",
    "    Age_Cat_input = Input(shape=(1,), name='Age_Cat')\n",
    "    AgeCat_emb = Embedding(5, 2, input_length=1, name='AgeCat_emb')(Age_Cat_input)\n",
    "    Flat4 = Reshape(target_shape=(2,))(AgeCat_emb)\n",
    "    inputs.append(Age_Cat_input)\n",
    "    embeddings.append(Flat4)\n",
    "\n",
    "    x = Concatenate()(embeddings)\n",
    "    x = Dense(14)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Activation('tanh')(x)\n",
    "    x = Dense(10)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Activation('tanh')(x)\n",
    "    x = Dense(6)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Activation('tanh')(x)\n",
    "    x = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    output = Dense(1, activation='exponential', trainable= False)(x)\n",
    "    \n",
    "    model = Model(inputs, output)\n",
    "    model.compile(optimizer=Nadam(0.004), loss='poisson')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e70000b",
   "metadata": {},
   "source": [
    "Then we define the model that takes the embeddings from the embedding network and uses it in learning the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define embedding model\n",
    "def build_embedding_model():\n",
    "    \n",
    "    inputs = []\n",
    "    embeddings = []\n",
    "    \n",
    "    pickle.load(open(str('EW1.dict'), \"rb\"))\n",
    "    pickle.load(open(str('EW2.dict'), \"rb\"))\n",
    "    pickle.load(open(str('EW3.dict'), \"rb\"))\n",
    "    pickle.load(open(str('EW4.dict'), \"rb\"))\n",
    "\n",
    "    Female_input = Input(shape=(1,), name='Female')\n",
    "    Fem_emb = Embedding(2, 1, input_length=1, name='Fem_emb', weights = [EW1])(Female_input)\n",
    "    Flat1 = Reshape(target_shape=(1,))(Fem_emb)\n",
    "    inputs.append(Female_input)\n",
    "    embeddings.append(Flat1)\n",
    "\n",
    "    Veh_type_input = Input(shape=(1,), name='Veh_type')\n",
    "    VehType_emb = Embedding(3, 2, input_length=1, name='VehType_emb', weights = [EW2])(Veh_type_input)\n",
    "    Flat2 = Reshape(target_shape=(2,))(VehType_emb)\n",
    "    inputs.append(Veh_type_input)\n",
    "    embeddings.append(Flat2)\n",
    "\n",
    "    Veh_age_input = Input(shape=(1,), name='Veh_age')\n",
    "    VehAge_emb = Embedding(4, 2, input_length=1, name='VehAge_emb', weights = [EW3])(Veh_age_input)\n",
    "    Flat3 = Reshape(target_shape=(2,))(VehAge_emb)\n",
    "    inputs.append(Veh_age_input)\n",
    "    embeddings.append(Flat3)\n",
    "\n",
    "    Age_Cat_input = Input(shape=(1,), name='Age_Cat')\n",
    "    AgeCat_emb = Embedding(5, 2, input_length=1, name='AgeCat_emb', weights = [EW4])(Age_Cat_input)\n",
    "    Flat4 = Reshape(target_shape=(2,))(AgeCat_emb)\n",
    "    inputs.append(Age_Cat_input)\n",
    "    embeddings.append(Flat4)\n",
    "\n",
    "    x = Concatenate()(embeddings)\n",
    "    x = Dense(14)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Activation('tanh')(x)\n",
    "    x = Dense(10)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Activation('tanh')(x)\n",
    "    x = Dense(6)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Activation('tanh')(x)\n",
    "    x = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    output = Dense(1, activation='exponential', trainable= False)(x)\n",
    "    \n",
    "    model = Model(inputs, output)\n",
    "    model.compile(optimizer=Nadam(0.004), loss='poisson')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a9ea5",
   "metadata": {},
   "source": [
    "We also need to define the function that converts data into a list format to match the embedding network structure and the embedding model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705cd8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data to list format to match the embedding model structure\n",
    "def preproc(Xtr, Xva, Xte):\n",
    "\n",
    "    input_list_train = []\n",
    "    input_list_val = []\n",
    "    input_list_test = []\n",
    "    \n",
    "    #the cols to be embedded: rescaling to range [0, # values)\n",
    "    for c in embed_cols:\n",
    "        raw_vals = np.unique(Xtr[c])\n",
    "        val_map = {}\n",
    "        for i in range(len(raw_vals)):\n",
    "            val_map[raw_vals[i]] = i       \n",
    "        input_list_train.append(Xtr[c].map(val_map).values)\n",
    "        input_list_val.append(Xva[c].map(val_map).fillna(0).values)\n",
    "        input_list_test.append(Xte[c].map(val_map).fillna(0).values)\n",
    "    \n",
    "    return input_list_train, input_list_val, input_list_test    \n",
    "\n",
    "\n",
    "#convert data to list format to match the embedding network\n",
    "def proproc(Xtr, Xva):\n",
    "\n",
    "    input_list_train = []\n",
    "    input_list_val = []\n",
    "    \n",
    "    #the cols to be embedded: rescaling to range [0, # values)\n",
    "    for c in embed_cols:\n",
    "        raw_vals = np.unique(Xtr[c])\n",
    "        val_map = {}\n",
    "        for i in range(len(raw_vals)):\n",
    "            val_map[raw_vals[i]] = i       \n",
    "        input_list_train.append(Xtr[c].map(val_map).values)\n",
    "        input_list_val.append(Xva[c].map(val_map).fillna(0).values)\n",
    "    \n",
    "    return input_list_train, input_list_val   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58963a24",
   "metadata": {},
   "source": [
    "We start with learning the embeddings using the embedding network. The embedding are saved and called back into the embedding model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a925d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "start = time.time()\n",
    "N = 1000\n",
    "BS = pd.DataFrame(columns=['PD','Error'])\n",
    "for i in range(N):\n",
    "    if (i % 200 == 0):\n",
    "        print(\"Iteration\", i)\n",
    "    #split training and testing data within source\n",
    "    \n",
    "    Xmy = mysource.drop(columns = ['N_Claims','Exposure'])\n",
    "    ymy = mysource['N_Claims']\n",
    "    vmy= mysource['Exposure']\n",
    "\n",
    "    idx_boot = np.random.choice(np.arange(len(Xmy)), len(Xmy), replace=True, p=vmy/vmy.sum())\n",
    "    Xmy_bs = Xmy.iloc[idx_boot]\n",
    "    ymy_bs = ymy.iloc[idx_boot]\n",
    "    vmy_bs = vmy.iloc[idx_boot]\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    idx1 = rng.choice(np.arange(len(Xmy_bs)), round(len(Xmy_bs)*0.8), replace=False)\n",
    "    idx2 = np.delete(np.arange(len(Xmy_bs)),idx1)\n",
    "\n",
    "    xs_sample = Xmy_bs.iloc[idx1]\n",
    "    ys_sample = ymy_bs.iloc[idx1]\n",
    "    vs_sample = vmy_bs.iloc[idx1]\n",
    "\n",
    "    Xvs = Xmy_bs.iloc[idx2]\n",
    "    yvs = ymy_bs.iloc[idx2]\n",
    "    vvs = vmy_bs.iloc[idx2]\n",
    "\n",
    "    cols_use = [c for c in xs_sample.columns if (not c.startswith('cat_'))]\n",
    "    xs_sample = xs_sample[cols_use]\n",
    "    col_vals_dict = {c: list(xs_sample[c].unique()) for c in xs_sample.columns}\n",
    "    embed_cols = []\n",
    "    for c in col_vals_dict:\n",
    "        if len(col_vals_dict[c])>1:\n",
    "            embed_cols.append(c)\n",
    "    \n",
    "    poc_Xts, poc_Xvs = proproc(xs_sample, Xvs)\n",
    "    \n",
    "    #learn embedding using source data\n",
    "    NNS = build_embedding_network()\n",
    "    NNS.fit(poc_Xts, ys_sample.values, epochs=300, callbacks=[callback], validation_data=(poc_Xvs,yvs.values), batch_size=512)\n",
    "    \n",
    "    #save the embeddings\n",
    "    EW1 = NNS.get_layer('Fem_emb').get_weights()[0]\n",
    "    pickle.dump(EW1, open(str('EW1.dict'), \"wb\"))\n",
    "    EW2 = NNS.get_layer('VehType_emb').get_weights()[0]\n",
    "    pickle.dump(EW2, open(str('EW2.dict'), \"wb\"))\n",
    "    EW3 = NNS.get_layer('VehAge_emb').get_weights()[0]\n",
    "    pickle.dump(EW3, open(str('EW3.dict'), \"wb\"))\n",
    "    EW4 = NNS.get_layer('AgeCat_emb').get_weights()[0]\n",
    "    pickle.dump(EW4, open(str('EW4.dict'), \"wb\"))\n",
    "    \n",
    "        \n",
    "    #split training and testing data within target\n",
    "    \n",
    "    Xty = mytarget.drop(columns = ['N_Claims','Exposure'])\n",
    "    yty = mytarget['N_Claims']\n",
    "    vty= mytarget['Exposure']\n",
    "\n",
    "    idx_boot1 = np.random.choice(np.arange(len(Xty)), len(Xty), replace=True, p=vty/vty.sum())\n",
    "    Xty_bs = Xty.iloc[idx_boot1]\n",
    "    yty_bs = yty.iloc[idx_boot1]\n",
    "    vty_bs = vty.iloc[idx_boot1]\n",
    "\n",
    "    rng1 = np.random.default_rng()\n",
    "    idx3 = rng1.choice(np.arange(len(Xty_bs)), round(len(Xty_bs)*0.6), replace=False)\n",
    "    idx4 = np.delete(np.arange(len(Xty_bs)),idx3)\n",
    "\n",
    "    xt_sample = Xty_bs.iloc[idx3]\n",
    "    yt_sample = yty_bs.iloc[idx3]\n",
    "    vt_sample = vty_bs.iloc[idx3]\n",
    "    \n",
    "    Xva_ty = Xty_bs.iloc[idx4]\n",
    "    yva_ty = yty_bs.iloc[idx4]\n",
    "    vva_ty = vty_bs.iloc[idx4]\n",
    "\n",
    "    rng2 = np.random.default_rng()\n",
    "    idx5 = rng2.choice(np.arange(len(Xva_ty)), round(len(Xva_ty)*0.625), replace=False)\n",
    "    idx6 = np.delete(np.arange(len(Xva_ty)),idx5)\n",
    "    \n",
    "    Xva = Xva_ty.iloc[idx5]\n",
    "    yva = yva_ty.iloc[idx5]\n",
    "    vva = vva_ty.iloc[idx5]\n",
    "    \n",
    "    Xte = Xva_ty.iloc[idx6]\n",
    "    yte = yva_ty.iloc[idx6]\n",
    "    vte = vva_ty.iloc[idx6]\n",
    "\n",
    "    cols_use = [c for c in xt_sample.columns if (not c.startswith('cat_'))]\n",
    "    xt_sample = xt_sample[cols_use]\n",
    "    col_vals_dict = {c: list(xt_sample[c].unique()) for c in xt_sample.columns}\n",
    "    embed_cols = []\n",
    "    for c in col_vals_dict:\n",
    "        if len(col_vals_dict[c])>1:\n",
    "            embed_cols.append(c)\n",
    "    \n",
    "    proc_Xtr, proc_Xva, proc_Xte = preproc(xt_sample, Xva, Xte)\n",
    "\n",
    "    #train on target data with embeddings learned from source data\n",
    "    NN = build_embedding_model()\n",
    "    NN.fit(proc_Xtr, yt_sample.values, epochs=300, callbacks=[callback], validation_data=(proc_Xva,yva.values), batch_size=512)\n",
    "    \n",
    "    TL_preds = NN.predict(proc_Xte)\n",
    "    deviance = mean_poisson_deviance(yte.values, TL_preds)\n",
    "    \n",
    "    BS.loc[i, ['PD']] = deviance\n",
    "    BS.loc[i, ['Error']] = error\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/60.0, \"min elapsed.\")\n",
    "\n",
    "print(BS.mean())\n",
    "print(BS.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203001ce",
   "metadata": {},
   "source": [
    "In other words, embeddings learned from the source data is transferred to the embedding model and is trained on the target data.\n",
    "\n",
    "The TR_emb model has a MPD of 0.4747(0.1630) with a MSE of 0.1112(0.0892) which is even worse than the Regularized Transfer model.\n",
    "\n",
    "A possible explanation for the bad performance of the Regularized Transfer model and the Transfer Embeddings model can be 'negative transfer', where transferring the learnings between dissimilar domains leads to worse performance than no transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5571f2c",
   "metadata": {},
   "source": [
    "In summary, instance-based and feature-based methods that have access to the target labels show better results than methods with no access to the target labels. In addition, adapting the distributional differences between the source and target by transforming features can provide relatively better predictions compared to transferring parameters from a source model and updating a target model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
